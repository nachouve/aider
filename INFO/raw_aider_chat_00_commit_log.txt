POST /v1/chat/completions HTTP/1.1
Host: api.openai.com
Accept-Encoding: gzip, deflate
Connection: keep-alive
Accept: application/json
Content-Type: application/json
User-Agent: OpenAI/Python 1.57.0
X-Stainless-Lang: python
X-Stainless-Package-Version: 1.57.0
X-Stainless-OS: Windows
X-Stainless-Arch: other:amd64
X-Stainless-Runtime: CPython
X-Stainless-Runtime-Version: 3.11.2
Authorization: Bearer sk-proj-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
X-Stainless-Async: false
X-Stainless-Raw-Response: true
x-stainless-retry-count: 0
Cookie: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Content-Length: 12026

{"messages": [{"role": "system", "content": "You are an expert software engineer that generates concise, one-line Git commit messages based on the provided diffs.\nReview the provided context and diffs which are about to be committed to a git repo.\nReview the diffs carefully.\nGenerate a one-line commit message for those changes.\nThe commit message should be structured as follows: <type>: <description>\nUse these for <type>: fix, feat, build, chore, ci, docs, style, refactor, perf, test\n\nEnsure the commit message:\n- Starts with the appropriate prefix.\n- Is in the imperative mood (e.g., \"Add feature\" not \"Added feature\" or \"Adding feature\").\n- Does not exceed 72 characters.\n\nReply only with the one-line commit message, without any additional text, explanations, or line breaks.\n"}, {"role": "user", "content": "\nUSER: improve \n\nASSISTANT: Para mejorar el archivo `custom_gc_proxy\\gc_proxy.py`, podemos realizar las siguientes optimizaciones:\n\n1. **Organizar las importaciones**: Agrupar las importaciones est\u00e1ndar, de terceros y locales.\n2. **Eliminar c\u00f3digo comentado**: Remover l\u00edneas de c\u00f3digo que est\u00e1n comentadas y no se utilizan.\n3. **Manejo de excepciones**: Mejorar el manejo de excepciones para proporcionar mensajes de error m\u00e1s claros.\n4. **Uso de constantes**: Definir constantes para valores repetidos o configuraciones.\n5. **Refactorizar funciones**: Simplificar funciones largas o complejas.\n\nAqu\u00ed est\u00e1n los cambios propuestos:\n\ncustom_gc_proxy\\gc_proxy.py\n```python\n<<<<<<< SEARCH\nfrom flask import Flask, request, jsonify\nimport requests\nimport os\nimport sys\nimport litellm\n\nlitellm.set_verbose=True\n\nfrom colorama import Fore\n\nimport dotenv\n=======\nimport os\nimport sys\n\nfrom flask import Flask, request, jsonify\nimport requests\nimport dotenv\nimport litellm\nfrom colorama import Fore\n\nlitellm.set_verbose = True\n>>>>>>> REPLACE\n```\n\ncustom_gc_proxy\\gc_proxy.py\n```python\n<<<<<<< SEARCH\n#sys.path.append(r\"C:\\Users\\nacho\\dev\\other\\iavarios\\ipython-copilot-completer\")\n\n=======\n>>>>>>> REPLACE\n```\n\ncustom_gc_proxy\\gc_proxy.py\n```python\n<<<<<<< SEARCH\n    # here the github authentification\n    gh_token = os.environ.get(\"GITHUB_COPILOT_ACCESS_TOKEN\", None) \n=======\n    gh_token = os.environ.get(\"GITHUB_COPILOT_ACCESS_TOKEN\")\n>>>>>>> REPLACE\n```\n\ncustom_gc_proxy\\gc_proxy.py\n```python\n<<<<<<< SEARCH\n    #TODO /completion use \"prompt\"  as parameter with a string... not messages=[{role: \"user\", content: \"Help me\"}, ..., ]\n    #TODO BUT..... INFO/row_copilot_requests.txt shows that it uses messages=[{role: \"user\", content: \"Help me\"}, ..., ] and not prompt!!!!!\n\n    # Get from ipython-copilot-completer\n    url = \"https://copilot-proxy.githubusercontent.com/v1/engines/copilot-codex/completions\"\n    #url = \"https://api.githubcopilot.com/chat/completions\"\n    \n\n    import ipdb; ipdb.set_trace()\n=======\n    url = \"https://copilot-proxy.githubusercontent.com/v1/engines/copilot-codex/completions\"\n>>>>>>> REPLACE\n```\n\ncustom_gc_proxy\\gc_proxy.py\n```python\n<<<<<<< SEARCH\n    # Before making the request, validate required parameters\n    #required_params = ['model', 'prompt', 'max_tokens', 'temperature', 'top_p']\n=======\n>>>>>>> REPLACE\n```\n\ncustom_gc_proxy\\gc_proxy.py\n```python\n<<<<<<< SEARCH\n    ## Ensure 'prompt' is provided as a list with at least one non-empty string\n    #if not isinstance(kwargs['prompt'], list) or not all(isinstance(item, str) and item.strip() for item in kwargs['prompt']):\n    #    raise ValueError(\"The 'prompt' parameter must be a list of non-empty strings.\")\n\n=======\n>>>>>>> REPLACE\n```\n\ncustom_gc_proxy\\gc_proxy.py\n```python\n<<<<<<< SEARCH\n    # # After validation, proceed with the request as before\n    # resp = requests.post(\n    #     url,\n    #     json={\n    #         \"model\": kwargs['model'],\n    #         \"params\": {\n    #             \"prompt\": kwargs['prompt'],\n    #             \"max_tokens\": kwargs['max_tokens'],\n    #             \"temperature\": kwargs['temperature'],\n    #             \"top_p\": kwargs['top_p'],\n    #             \"top_k\": kwargs.get(\"top_k\", 40),\n    #         },\n    #     },\n    #     verify=False,\n    # )\n\n=======\n>>>>>>> REPLACE\n```\n\ncustom_gc_proxy\\gc_proxy.py\n```python\n<<<<<<< SEARCH\n    # headers = {\n    #     \"OpenAI-Intent\": \"conversation-panel\",\n    #     \"OpenAI-Organization\": \"github-copilot\",\n    #     ### Each call a new token? \n    #     \"Authorization\": f\"Bearer {get_copilot_token()}\",\n    #     \"Content-Type\": \"application/json\",\n    #     \"user-agent\": \"GitHubCopilotChat/0.15.1\",\n    #     \"editor-version\": \"vscode/1.89.1\",\n    #     \"editor-plugin-version\": \"copilot-chat/0.15.1\",\n    #     \"x-request-id\": \"b36f22a9-88f0-47fc-bfb8-47bc6273adf6\",\n    #     \"x-github-api-version\": \"2023-07-07\",\n    #     \"openai-organization\": \"github-copilot\",\n    #     \"copilot-integration-id\": \"vscode-chat\",\n    #     \"vscode-sessionid\": \"204ccf01-6c28-42b0-8c8f-2d453199af051715499526505\",\n    #     \"vscode-machineid\": \"ccc60edbc08ad1aac1be74576b329b10cdc2c094555f873711e3ca1a2e0f6afe\",\n    # }\n\n=======\n>>>>>>> REPLACE\n```\n\ncustom_gc_proxy\\gc_proxy.py\n```python\n<<<<<<< SEARCH\n    #messages = kwargs.get(\"messages\") + [{\"role\": \"user\", \"content\": \"Help me with what I've asked for.\"}]\n=======\n>>>>>>> REPLACE\n```\n\ncustom_gc_proxy\\gc_proxy.py\n```python\n<<<<<<< SEARCH\n    # payload = {\n    #     \"intent\": True,\n    #     \"max_tokens\": 250,\n    #     \"model\": MODEL,\n    #     \"n\": 1,\n    #     \"stream\": True,\n    #     \"temperature\": 0.1,\n    #     \"top_p\": 1,\n    #     \"messages\": messages \n    # }\n=======\n>>>>>>> REPLACE\n```\n\nEstos cambios mejoran la legibilidad y el mantenimiento del c\u00f3digo al eliminar comentarios innecesarios y organizar las importaciones. Adem\u00e1s, se mejora el manejo de excepciones y se asegura que las configuraciones se manejen de manera m\u00e1s clara.\n\n# Diffs:\ndiff --git a/custom_gc_proxy/gc_proxy.py b/custom_gc_proxy/gc_proxy.py\nindex c625e2bdf..227243523 100644\n--- a/custom_gc_proxy/gc_proxy.py\n+++ b/custom_gc_proxy/gc_proxy.py\n@@ -1,14 +1,13 @@\n-from flask import Flask, request, jsonify\n-import requests\n import os\n import sys\n-import litellm\n-\n-litellm.set_verbose=True\n \n+from flask import Flask, request, jsonify\n+import requests\n+import dotenv\n+import litellm\n from colorama import Fore\n \n-import dotenv\n+litellm.set_verbose = True\n \n env_locations = [\".env\", \"aider/.env\", \"../.env\", \"../aider/.env\", \"../aider/aider/.env\"]\n for env_location in env_locations:\n@@ -19,7 +18,6 @@ if not os.getenv(\"GITHUB_COPILOT_ACCESS_TOKEN\"):\n     print(f\"{Fore.RED}GITHUB_COPILOT_ACCESS_TOKEN not set{Fore.RESET}\")\n     sys.exit(1)\n \n-#sys.path.append(r\"C:\\Users\\nacho\\dev\\other\\iavarios\\ipython-copilot-completer\")\n \n app = Flask(__name__)\n \n@@ -31,8 +29,7 @@ from copilot_completer.settings import Settings\n from copilot_completer.github_auth import get_github_access_token\n from copilot_completer.completer import get_copilot_token\n def copilot_login():\n-    # here the github authentification\n-    gh_token = os.environ.get(\"GITHUB_COPILOT_ACCESS_TOKEN\", None) \n+    gh_token = os.environ.get(\"GITHUB_COPILOT_ACCESS_TOKEN\")\n     settings = Settings.from_env()\n     if gh_token is None:\n         print(\"Need to login to GitHub Copilot. See instructions in the internet browser.\")\n@@ -88,18 +85,8 @@ def chat_completions():\n \n @app.route('/chat/completions', methods=['POST'])\n def completions():\n-    #TODO /completion use \"prompt\"  as parameter with a string... not messages=[{role: \"user\", content: \"Help me\"}, ..., ]\n-    #TODO BUT..... INFO/row_copilot_requests.txt shows that it uses messages=[{role: \"user\", content: \"Help me\"}, ..., ] and not prompt!!!!!\n-\n-    # Get from ipython-copilot-completer\n     url = \"https://copilot-proxy.githubusercontent.com/v1/engines/copilot-codex/completions\"\n-    #url = \"https://api.githubcopilot.com/chat/completions\"\n-    \n-\n-    import ipdb; ipdb.set_trace()\n     kwargs = request.json\n-    # Before making the request, validate required parameters\n-    #required_params = ['model', 'prompt', 'max_tokens', 'temperature', 'top_p']\n     required_params = ['model', 'max_tokens', 'temperature', 'top_p']\n     missing_params = [param for param in required_params if param not in kwargs]\n     if missing_params:\n@@ -109,9 +96,6 @@ def completions():\n     if not isinstance(kwargs['model'], str) or not kwargs['model'].strip():\n         raise ValueError(\"The 'model' parameter must be a non-empty string.\")\n \n-    ## Ensure 'prompt' is provided as a list with at least one non-empty string\n-    #if not isinstance(kwargs['prompt'], list) or not all(isinstance(item, str) and item.strip() for item in kwargs['prompt']):\n-    #    raise ValueError(\"The 'prompt' parameter must be a list of non-empty strings.\")\n \n     # Ensure 'max_tokens', 'temperature', and 'top_p' are of correct type and within expected ranges\n     if not isinstance(kwargs['max_tokens'], int) or kwargs['max_tokens'] <= 0:\n@@ -123,41 +107,10 @@ def completions():\n     if not isinstance(kwargs['top_p'], (int, float)) or not (0 <= kwargs['top_p'] <= 1):\n         raise ValueError(\"The 'top_p' parameter must be a number between 0 and 1.\")\n \n-    # # After validation, proceed with the request as before\n-    # resp = requests.post(\n-    #     url,\n-    #     json={\n-    #         \"model\": kwargs['model'],\n-    #         \"params\": {\n-    #             \"prompt\": kwargs['prompt'],\n-    #             \"max_tokens\": kwargs['max_tokens'],\n-    #             \"temperature\": kwargs['temperature'],\n-    #             \"top_p\": kwargs['top_p'],\n-    #             \"top_k\": kwargs.get(\"top_k\", 40),\n-    #         },\n-    #     },\n-    #     verify=False,\n-    # )\n \n     copilot_login()\n \n \n-    # headers = {\n-    #     \"OpenAI-Intent\": \"conversation-panel\",\n-    #     \"OpenAI-Organization\": \"github-copilot\",\n-    #     ### Each call a new token? \n-    #     \"Authorization\": f\"Bearer {get_copilot_token()}\",\n-    #     \"Content-Type\": \"application/json\",\n-    #     \"user-agent\": \"GitHubCopilotChat/0.15.1\",\n-    #     \"editor-version\": \"vscode/1.89.1\",\n-    #     \"editor-plugin-version\": \"copilot-chat/0.15.1\",\n-    #     \"x-request-id\": \"b36f22a9-88f0-47fc-bfb8-47bc6273adf6\",\n-    #     \"x-github-api-version\": \"2023-07-07\",\n-    #     \"openai-organization\": \"github-copilot\",\n-    #     \"copilot-integration-id\": \"vscode-chat\",\n-    #     \"vscode-sessionid\": \"204ccf01-6c28-42b0-8c8f-2d453199af051715499526505\",\n-    #     \"vscode-machineid\": \"ccc60edbc08ad1aac1be74576b329b10cdc2c094555f873711e3ca1a2e0f6afe\",\n-    # }\n \n     print(\"DEBUG>>>> get_copilot_token()\")\n     COPILOT_TOKEN = get_copilot_token()\n@@ -211,19 +164,8 @@ def completions():\n             },\n         ]\n \n-    #messages = kwargs.get(\"messages\") + [{\"role\": \"user\", \"content\": \"Help me with what I've asked for.\"}]\n     print(messages[-1])\n          \n-    # payload = {\n-    #     \"intent\": True,\n-    #     \"max_tokens\": 250,\n-    #     \"model\": MODEL,\n-    #     \"n\": 1,\n-    #     \"stream\": True,\n-    #     \"temperature\": 0.1,\n-    #     \"top_p\": 1,\n-    #     \"messages\": messages \n-    # }\n     print(\"litellm.completion()...\")\n \n     kwargs.update({\"messages\": messages})\n@@ -257,4 +199,4 @@ def embeddings():\n     return forward_request('/embeddings')\n \n if __name__ == \"__main__\":\n-    app.run(debug=True)\n\\ No newline at end of file\n+    app.run(debug=True)"}], "model": "gpt-4o-mini", "stream": false, "temperature": 0}

HTTP/1.1 200 OK
Date: Tue, 21 Jan 2025 20:40:47 GMT
Content-Type: application/json
Transfer-Encoding: chunked
Connection: keep-alive
access-control-expose-headers: X-Request-ID
openai-organization: nachouvefreelance
openai-processing-ms: 505
openai-version: 2020-10-01
x-ratelimit-limit-requests: 10000
x-ratelimit-limit-tokens: 200000
x-ratelimit-remaining-requests: 9999
x-ratelimit-remaining-tokens: 197175
x-ratelimit-reset-requests: 8.64s
x-ratelimit-reset-tokens: 847ms
x-request-id: req_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
strict-transport-security: max-age=31536000; includeSubDomains; preload
CF-Cache-Status: DYNAMIC
X-Content-Type-Options: nosniff
Server: cloudflare
CF-RAY: 905a1f09ea602f82-MAD
alt-svc: h3=":443"; ma=86400

350
{
  "id": "chatcmpl-AsFLC95LddBoa2EHX1nkMwkIPDjdx",
  "object": "chat.completion",
  "created": 1737492046,
  "model": "gpt-4o-mini-2024-07-18",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "style: Organize imports and remove commented code in gc_proxy.py",
        "refusal": null
      },
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 3127,
    "completion_tokens": 14,
    "total_tokens": 3141,
    "prompt_tokens_details": {
      "cached_tokens": 0,
      "audio_tokens": 0
    },
    "completion_tokens_details": {
      "reasoning_tokens": 0,
      "audio_tokens": 0,
      "accepted_prediction_tokens": 0,
      "rejected_prediction_tokens": 0
    }
  },
  "service_tier": "default",
  "system_fingerprint": "fp_72ed7ab54c"
}

0

